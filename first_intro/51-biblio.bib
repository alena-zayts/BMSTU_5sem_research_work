@article{sikka_survey_2020,
	title = {A Survey on Text Simplification},
	url = {http://arxiv.org/abs/2008.08612},
	abstract = {Text Simplification ({TS}) aims to reduce the linguistic complexity of content to make it easier to understand. Research in {TS} has been of keen interest, especially as approaches to {TS} have shifted from manual, hand-crafted rules to automated simplification. This survey seeks to provide a comprehensive overview of {TS}, including a brief description of earlier approaches used, discussion of various aspects of simplification (lexical, semantic and syntactic), and latest techniques being utilized in the field. We note that the research in the field has clearly shifted towards utilizing deep learning techniques to perform {TS}, with a specific focus on developing solutions to combat the lack of data available for simplification. We also include a discussion of datasets and evaluations metrics commonly used, along with discussion of related fields within Natural Language Processing ({NLP}), like semantic similarity.},
	journaltitle = {{arXiv}:2008.08612 [cs]},
	author = {Sikka, Punardeep and Mago, Vijay},
	urldate = {2021-11-03},
	date = {2020-08-24},
	eprinttype = {arxiv},
	eprint = {2008.08612},
	keywords = {Computer Science - Computation and Language},
	file = {Sikka_Mago_2020_A Survey on Text Simplification.pdf:C\:\\Users\\alena\\Zotero\\storage\\W64CWQ8P\\Sikka_Mago_2020_A Survey on Text Simplification.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\alena\\Zotero\\storage\\2544D53M\\2008.html:text/html},
}


@article{finegan_dollak_sentence_2016,
	title = {Sentence simplification, compression, and disaggregation for summarization of sophisticated documents},
	volume = {67},
	issn = {23301635},
	url = {https://onlinelibrary.wiley.com/doi/10.1002/asi.23576},
	doi = {10.1002/asi.23576},
	pages = {2437--2453},
	number = {10},
	journaltitle = {Journal of the Association for Information Science and Technology},
	shortjournal = {J Assn Inf Sci Tec},
	author = {Finegan-Dollak, Catherine and Radev, Dragomir R.},
	urldate = {2021-10-27},
	date = {2016-10},
	langid = {english},
	file = {Finegan-Dollak_Radev_2016_Sentence simplification, compression, and disaggregation for summarization of.pdf:C\:\\Users\\alena\\Zotero\\storage\\8KP7EYHZ\\Finegan-Dollak_Radev_2016_Sentence simplification, compression, and disaggregation for summarization of.pdf:application/pdf},
}


@inproceedings{liu_simplification_2016,
	location = {Osaka, Japan},
	title = {Simplification of Example Sentences for Learners of Japanese Functional Expressions},
	url = {https://aclanthology.org/W16-4901},
	abstract = {Learning functional expressions is one of the difficulties for language learners, since functional expressions tend to have multiple meanings and complicated usages in various situations. In this paper, we report an experiment of simplifying example sentences of Japanese functional expressions especially for Chinese-speaking learners. For this purpose, we developed “Japanese Functional Expressions List” and “Simple Japanese Replacement List”. To evaluate the method, we conduct a small-scale experiment with Chinese-speaking learners on the effectiveness of the simplified example sentences. The experimental results indicate that simplified sentences are helpful in learning Japanese functional expressions.},
	pages = {1--5},
	booktitle = {Proceedings of the 3rd Workshop on Natural Language Processing Techniques for Educational Applications ({NLPTEA}2016)},
	publisher = {The {COLING} 2016 Organizing Committee},
	author = {Liu, Jun and Matsumoto, Yuji},
	urldate = {2021-11-03},
	date = {2016-12},
	keywords = {введение},
	file = {Liu_Matsumoto_2016_Simplification of Example Sentences for Learners of Japanese Functional.pdf:C\:\\Users\\alena\\Zotero\\storage\\J4JJ977Q\\Liu_Matsumoto_2016_Simplification of Example Sentences for Learners of Japanese Functional.pdf:application/pdf},
}


@inproceedings{evans_evaluation_2014,
	location = {Gothenburg, Sweden},
	title = {An evaluation of syntactic simplification rules for people with autism},
	url = {https://aclanthology.org/W14-1215},
	doi = {10.3115/v1/W14-1215},
	pages = {131--140},
	booktitle = {Proceedings of the 3rd Workshop on Predicting and Improving Text Readability for Target Reader Populations ({PITR})},
	publisher = {Association for Computational Linguistics},
	author = {Evans, Richard and Orăsan, Constantin and Dornescu, Iustin},
	urldate = {2021-11-03},
	date = {2014-04},
	keywords = {введение},
	file = {Evans et al_2014_An evaluation of syntactic simplification rules for people with autism.pdf:C\:\\Users\\alena\\Zotero\\storage\\4PQKMDAR\\Evans et al_2014_An evaluation of syntactic simplification rules for people with autism.pdf:application/pdf},
}


@inproceedings{kazan_federal_university,
	title = {{RuSimpleSentEval}-2021 Shared Task: Evaluating Sentence Simplification for Russian},
	url = {http://www.dialog-21.ru/media/5558/sakhovskiyaplusetal161.pdf},
	doi = {10.28995/2075-7182-2021-20-607-617},
	shorttitle = {{RuSimpleSentEval}-2021 Shared Task},
	abstract = {This report presents the results from the {RuSimpleSentEval} Shared Task conducted as a part of the Dialogue 2021 evaluation campaign. For the {RSSE} Shared Task, devoted to sentence simpliﬁcation in Russian, a new middlescale dataset is created from scratch. It enumerates more than 3000 sentences sampled from popular Wikipedia pages. Each sentence is aligned with 2.2 simpliﬁed modiﬁcations, on average. The Shared Task implies sequenceto-sequence approaches: given an input complex sentence, a system should provide with its simpliﬁed version. A popular sentence simpliﬁcation measure, {SARI}, is used to evaluate the system’s performance.},
	eventtitle = {Computational Linguistics and Intellectual Technologies},
	pages = {607--617},
	author = {{Kazan Federal University, Kazan, Russia} and Sakhovskiy, Andrey and Izhevskaya, Alexandra and {National Research University Higher School of Economics, Moscow, Russia} and Pestova, Alena and {National Research University Higher School of Economics, Moscow, Russia} and Tutubalina, Elena and {Kazan Federal University, Kazan, Russia} and Malykh, Valentin and {Kazan Federal University, Kazan, Russia} and Smurov, Ivan and {ABBYY, Moscow, Russia} and Artemova, Ekaterina and {National Research University Higher School of Economics, Moscow, Russia}},
	urldate = {2021-10-27},
	date = {2021-06-19},
	langid = {english},
	file = {Kazan Federal University, Kazan, Russia и др. - 2021 - RuSimpleSentEval-2021 Shared Task Evaluating Sent.pdf:C\:\\Users\\alena\\Zotero\\storage\\3JA4UQ33\\Kazan Federal University, Kazan, Russia и др. - 2021 - RuSimpleSentEval-2021 Shared Task Evaluating Sent.pdf:application/pdf},
}


@inproceedings{galeev_rubts_2021,
	title = {{ruBTS}: Russian Sentence Simplification Using Back-translation},
	url = {http://www.dialog-21.ru/media/5510/galeevfplusleushinamplusivanovv144.pdf},
	doi = {10.28995/2075-7182-2021-20-259-267},
	shorttitle = {{ruBTS}},
	abstract = {Automatic text simpliﬁcation is a crucial task enabling to reduce text complexity while preserving meaning. This paper presents our solution to the Russian Sentence Simpliﬁcation Shared Task ({RSSE}) based on a backtranslation technique. We show that applying the simple back-translation approach for sentence simpliﬁcation can give competitive results with the other methods without ﬁne-tuning or training.},
	eventtitle = {Computational Linguistics and Intellectual Technologies},
	pages = {259--267},
	author = {Galeev, Farit and Leushina, Marina and Ivanov, Vladimir and {Innopolis University Innopolis, Russia}},
	urldate = {2021-10-27},
	date = {2021-06-19},
	langid = {english},
	file = {Galeev и др. - 2021 - ruBTS Russian Sentence Simplification Using Back-.pdf:C\:\\Users\\alena\\Zotero\\storage\\5LQ9CHUM\\Galeev и др. - 2021 - ruBTS Russian Sentence Simplification Using Back-.pdf:application/pdf},
}


@article{xu_optimizing_2016,
	title = {Optimizing Statistical Machine Translation for Text Simplification},
	volume = {4},
	issn = {2307-387X},
	url = {https://doi.org/10.1162/tacl\_a\_00107},
	doi = {10.1162/tacl\_a\_00107},
	abstract = {Most recent sentence simplification systems use basic machine translation models
	to learn lexical and syntactic paraphrases from a manually simplified parallel
	corpus. These methods are limited by the quality and quantity of manually
	simplified corpora, which are expensive to build. In this paper, we conduct an
	in-depth adaptation of statistical machine translation to perform text
	simplification, taking advantage of large-scale paraphrases learned from
	bilingual texts and a small amount of manual simplifications with multiple
	references. Our work is the first to design automatic metrics that are effective
	for tuning and evaluating simplification systems, which will facilitate
	iterative development for this task.},
	pages = {401--415},
	journaltitle = {Transactions of the Association for Computational Linguistics},
	shortjournal = {Transactions of the Association for Computational Linguistics},
	author = {Xu, Wei and Napoles, Courtney and Pavlick, Ellie and Chen, Quanze and Callison-Burch, Chris},
	urldate = {2021-11-03},
	date = {2016-07-01},
	keywords = {sari},
	file = {Xu et al_2016_Optimizing Statistical Machine Translation for Text Simplification.pdf:C\:\\Users\\alena\\Zotero\\storage\\3UQKHJI2\\Xu et al_2016_Optimizing Statistical Machine Translation for Text Simplification.pdf:application/pdf;Snapshot:C\:\\Users\\alena\\Zotero\\storage\\INVEULJ3\\Optimizing-Statistical-Machine-Translation-for.html:text/html},
}


@article{martin_muss_2021,
	title = {{MUSS}: Multilingual Unsupervised Sentence Simplification by Mining Paraphrases},
	url = {http://arxiv.org/abs/2005.00352},
	shorttitle = {{MUSS}},
	abstract = {Progress in sentence simplification has been hindered by a lack of labeled parallel simplification data, particularly in languages other than English. We introduce {MUSS}, a Multilingual Unsupervised Sentence Simplification system that does not require labeled simplification data. {MUSS} uses a novel approach to sentence simplification that trains strong models using sentence-level paraphrase data instead of proper simplification data. These models leverage unsupervised pretraining and controllable generation mechanisms to flexibly adjust attributes such as length and lexical complexity at inference time. We further present a method to mine such paraphrase data in any language from Common Crawl using semantic sentence embeddings, thus removing the need for labeled data. We evaluate our approach on English, French, and Spanish simplification benchmarks and closely match or outperform the previous best supervised results, despite not using any labeled simplification data. We push the state of the art further by incorporating labeled simplification data.},
	journaltitle = {{arXiv}:2005.00352 [cs]},
	author = {Martin, Louis and Fan, Angela and de la Clergerie, Éric and Bordes, Antoine and Sagot, Benoît},
	urldate = {2021-11-03},
	date = {2021-04-16},
	eprinttype = {arxiv},
	eprint = {2005.00352},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning},
	file = {Martin et al_2021_MUSS.pdf:C\:\\Users\\alena\\Zotero\\storage\\422U6HDR\\Martin et al_2021_MUSS.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\alena\\Zotero\\storage\\73NM8YSC\\2005.html:text/html},
}


@inproceedings{fang_learning_2017,
	location = {Copenhagen, Denmark},
	title = {Learning how to Active Learn: A Deep Reinforcement Learning Approach},
	url = {https://aclanthology.org/D17-1063},
	doi = {10.18653/v1/D17-1063},
	shorttitle = {Learning how to Active Learn},
	abstract = {Active learning aims to select a small subset of data for annotation such that a classifier learned on the data is highly accurate. This is usually done using heuristic selection methods, however the effectiveness of such methods is limited and moreover, the performance of heuristics varies between datasets. To address these shortcomings, we introduce a novel formulation by reframing the active learning as a reinforcement learning problem and explicitly learning a data selection policy, where the policy takes the role of the active learning heuristic. Importantly, our method allows the selection policy learned using simulation to one language to be transferred to other languages. We demonstrate our method using cross-lingual named entity recognition, observing uniform improvements over traditional active learning algorithms.},
	eventtitle = {{EMNLP} 2017},
	pages = {595--605},
	booktitle = {Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing},
	publisher = {Association for Computational Linguistics},
	author = {Fang, Meng and Li, Yuan and Cohn, Trevor},
	urldate = {2021-11-03},
	date = {2017-09},
	keywords = {dress},
	file = {Fang et al_2017_Learning how to Active Learn.pdf:C\:\\Users\\alena\\Zotero\\storage\\788K7AIH\\Fang et al_2017_Learning how to Active Learn.pdf:application/pdf},
}



@article{voznesenskaya_automatic_2018,
	title = {Automatic text summarization system using a stochastic model},
	volume = {4},
	issn = {22233792},
	url = {http://jmlda.org/papers/doc/2018/no4/Voznesenskaya2018.pdf},
	doi = {10.21469/22233792.4.4.04},
	pages = {266--279},
	number = {4},
	journaltitle = {Machine Learning and Data Analysis},
	shortjournal = {{JMLDA}},
	author = {Voznesenskaya, Tamara},
	urldate = {2021-10-25},
	date = {2018-12-29},
	langid = {russian},
	file = {Voznesenskaya - 2018 - Automatic text summarization system using a stocha.pdf:C\:\\Users\\alena\\Zotero\\storage\\Z4JQ5RUQ\\Voznesenskaya - 2018 - Automatic text summarization system using a stocha.pdf:application/pdf},
}


@inproceedings{kazan_federal_university_kazan_russia_rusimplesenteval-2021_2021,
	title = {{RuSimpleSentEval}-2021 Shared Task: Evaluating Sentence Simplification for Russian},
	url = {http://www.dialog-21.ru/media/5558/sakhovskiyaplusetal161.pdf},
	doi = {10.28995/2075-7182-2021-20-607-617},
	shorttitle = {{RuSimpleSentEval}-2021 Shared Task},
	abstract = {This report presents the results from the {RuSimpleSentEval} Shared Task conducted as a part of the Dialogue 2021 evaluation campaign. For the {RSSE} Shared Task, devoted to sentence simpliﬁcation in Russian, a new middlescale dataset is created from scratch. It enumerates more than 3000 sentences sampled from popular Wikipedia pages. Each sentence is aligned with 2.2 simpliﬁed modiﬁcations, on average. The Shared Task implies sequenceto-sequence approaches: given an input complex sentence, a system should provide with its simpliﬁed version. A popular sentence simpliﬁcation measure, {SARI}, is used to evaluate the system’s performance.},
	eventtitle = {Computational Linguistics and Intellectual Technologies},
	pages = {607--617},
	author = {{Kazan Federal University, Kazan, Russia} and Sakhovskiy, Andrey and Izhevskaya, Alexandra and {National Research University Higher School of Economics, Moscow, Russia} and Pestova, Alena and {National Research University Higher School of Economics, Moscow, Russia} and Tutubalina, Elena and {Kazan Federal University, Kazan, Russia} and Malykh, Valentin and {Kazan Federal University, Kazan, Russia} and Smurov, Ivan and {ABBYY, Moscow, Russia} and Artemova, Ekaterina and {National Research University Higher School of Economics, Moscow, Russia}},
	urldate = {2021-10-27},
	date = {2021-06-19},
	langid = {english},
	file = {Kazan Federal University, Kazan, Russia и др. - 2021 - RuSimpleSentEval-2021 Shared Task Evaluating Sent.pdf:C\:\\Users\\alena\\Zotero\\storage\\3JA4UQ33\\Kazan Federal University, Kazan, Russia и др. - 2021 - RuSimpleSentEval-2021 Shared Task Evaluating Sent.pdf:application/pdf},
}





@online{noauthor_bart_nodate,
	title = {{BART}: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension - {ACL} Anthology},
	url = {https://aclanthology.org/2020.acl-main.703/},
	urldate = {2021-11-25},
	file = {BART\: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension - ACL Anthology:C\:\\Users\\alena\\Zotero\\storage\\LIXMXVHL\\2020.acl-main.703.html:text/html;1910.13461.pdf:C\:\\Users\\alena\\Zotero\\storage\\X9IYWC78\\1910.13461.pdf:application/pdf},
}

@online{noauthor_191013461_nodate,
	title = {[1910.13461] {BART}: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension},
	url = {https://arxiv.org/abs/1910.13461},
	urldate = {2021-11-25},
	file = {[1910.13461] BART\: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension:C\:\\Users\\alena\\Zotero\\storage\\AG2GFCBM\\1910.html:text/html},
}

@inproceedings{saint_petersburg_russia_rusimscore_2021,
	title = {{RuSimScore}: unsupervised scoring function for Russian sentence simplification quality},
	url = {http://www.dialog-21.ru/media/5535/orzhenovskiimv143.pdf},
	doi = {10.28995/2075-7182-2021-20-524-532},
	shorttitle = {{RuSimScore}},
	abstract = {We propose an unsupervised complex scoring function ({RuSimScore}) to measure simpliﬁcation quality of Russian sentences, and a model for text simpliﬁcation based on this function. The function allows to score simplicity and original meaning preservation. First, ﬁltered a noisy parallel corpus (machine translated {WikiLarge}) and extracted good simpliﬁcation examples. After that, a pretrained language model was ﬁne-tuned on these examples. We generate multiple outputs from the language model and select the best one according to the scoring function. The weights in the scoring function can be adjusted to balance between better content preservation and getting simpler sentences (controllable simpliﬁcation).},
	eventtitle = {Computational Linguistics and Intellectual Technologies},
	pages = {524--532},
	author = {{Saint Petersburg, Russia} and Orzhenovskii, Mikhail},
	urldate = {2021-11-27},
	date = {2021-06-19},
	langid = {english},
	file = {Saint Petersburg, Russia и Orzhenovskii - 2021 - RuSimScore unsupervised scoring function for Russ.pdf:C\:\\Users\\alena\\Zotero\\storage\\QK86TET9\\Saint Petersburg, Russia и Orzhenovskii - 2021 - RuSimScore unsupervised scoring function for Russ.pdf:application/pdf},
}