
В рамках выполнения работы необходимо решить следующие задачи: 
\begin{enumerate}[label={\arabic*)}]
	\item собрать или найти крупный параллельный корпус на русском языке, состоящий из сложных предложений и их выровненных упрощенных версий и основанный на разных типах источников;
	\item подобрать метрики, по которым можно определить качество упрощения;
	\item изучить существующие подходы к решению задачи упрощения текстов на уровне предложений как на русском, так и на иностранных языках;
	\item разработать или адаптировать к русскому языку несколько моделей, решающих эту задачу;
	\item провести сравнительный анализ разработанных моделей, обученных как на всем собранном корпусе, так и на его отдельных частях (основанных на разных типах источников);
	\item выбрать наилучшую модель.
\end{enumerate}


%\textbf{\textit{•	методы исследования}}\\


Упрощение предложений можно рассматривать как задачу <<от последовательности к последовательности>> (sequence-to-sequence, seq2seq)~\cite{kazan_federal_university}, к которой применимы нейронные модели языка. Такая модель получает на вход исходное предложение, а на выходе выдает его упрощенную версию.

(!В статье "A Survey on Text Simplification" выделяется множество подходов. Почему Вы вынесли сюда именно его? Детально анализировать все подходы нужно будет, конечно, не во Введении, а в Аналитическом разделе ВКР (ну или, я так понимаю, в Вашем случае - в самой НИР), но во Введении нужно хотя бы рассказать, как вообще современное научное сообщество понимает "упрощение" текста и кратко, обобщённо пересказать общую суть разных групп подходов к этой задаче.)





В последнее время достигнуты хорошие результаты в решении проблемы упрощения предложений. Модель-трансформер mBART, первоначально разработанная для машинного перевода, оказалась эффективной для решения задач такого рода, в особенности при добавлении специальных маркеров управления ~\cite{martin_muss_2021}.

В основе большинства современных моделей лежит подход encoder-decoder (кодировщика-декодера), зачастую дополненный и другими инструментами. Например, модель DRESS ~\cite{fang_learning_2017} добавляет в архитектуру encoder-decoder подход обучения с подкреплением: выполняется обзор возможных упрощений и выбирается наилучший из них.
 
 Решение, победившее в соревновании, организованном в рамках конференции DIALOGUE 2021,  в значительной степени основано на системе MUSS (Multilingual Unsupervised Sentence Simplification, система мультиязычного упрощения, обучающаяся без учителя) ~\cite{martin_muss_2021}. Модель состоит из mBART, настроенного на корпусах ParaPhraserPlus\footnote{\url{https://metatext.io/datasets/paraphraser-plus}} (пары предложений на русском языке, разделенные на 3 класса по степени "перефразированности") и RuWikiSimple, в которую добалены управляющие токены (расстояние Левенштейна, доля совпадающих символов между оригинальным и упрощенным предложениями, сходство лексем).
 
 Модели, занявшие следующие места - генеративные модели на основе GPT, настроенные на отфильтрованном корпусе RuWikiSimple.
 
 (!Не хватает какого-то завершения Введения. Например, в статье "Система автоматического аннотирования текстов с
 помощью стохастической модели" Введение заканчивается словами:
 "Таким образом, на сегодняшний день оценка качества аннотирования не обходится без
 работы экспертов, что безусловно дорого и требует существенных временных затрат."
 
 Надо как-то подвести итог, в каком состоянии находится сейчас решаемая задача и обозначить место Вашей работы в текущем положении вещей.)

~\cite{martin_muss_2021}
\cite{voznesenskaya_automatic_2018}
~\cite{kazan_federal_university_kazan_russia_rusimplesenteval-2021_2021}
~\cite{noauthor_bart_nodate}
~\cite{noauthor_191013461_nodate}
~\cite{saint_petersburg_russia_rusimscore_2021}






